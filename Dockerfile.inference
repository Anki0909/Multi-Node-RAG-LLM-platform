FROM nvidia/cuda:12.2.0-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
WORKDIR /app

RUN apt-get update && apt-get install -y \
    git cmake build-essential \
    python3 python3-pip \
    && rm -rf /var/lib/apt/lists/*

RUN git clone https://github.com/ggerganov/llama.cpp.git && \
    cd llama.cpp && \
    cmake -B build \
      -DLLAMA_CUDA=ON \
      -DLLAMA_BUILD_EXAMPLES=OFF \
      -DLLAMA_BUILD_TESTS=OFF \
      -DLLAMA_BUILD_SERVER=OFF \
      -DLLAMA_BUILD_TOOLS=OFF && \
    cmake --build build -j$(nproc)

ENV MODEL_PATH=/mnt/data/rag/models
ENV CACHE_PATH=/mnt/data/rag/cache

EXPOSE 8000

CMD ["bash", "-c", \
"cd llama.cpp && ./build/bin/server \
  --model $MODEL_PATH/qwen2.5-3b-instruct-q4_k_m.gguf \
  --ctx-size 4096 \
  --n-gpu-layers 28 \
  --host 0.0.0.0 \
  --port 8000"]
