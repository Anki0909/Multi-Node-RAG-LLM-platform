apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-inference
  namespace: rag-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rag-inference
  template:
    metadata:
      labels:
        app: rag-inference
    spec:
      nodeSelector:
        rag-role: compute
      containers:
        - name: rag-inference
          image: rag-inference:v0.5-amd64
          imagePullPolicy: Never
          ports:
            - containerPort: 8000
          envFrom:
            - configMapRef:
                name: rag-config
          resources:
            requests:
              cpu: "2"
              memory: "6Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
          volumeMounts:
            - name: llm-models
              mountPath: /models
            - name: vector-db
              mountPath: /data/vector-db
          env:
            - name: VECTOR_DB_PATH
              value: /data/vector-db
      volumes:
        - name: llm-models
          persistentVolumeClaim:
            claimName: llm-models-pvc
        - name: vector-db
          persistentVolumeClaim:
            claimName: vector-db-pvc